[{"content":"As the analysis of our web archives collected during the pandemic draws to a close, it is time to share some of the work that we have produced. Instead of analysing the entire 300 GB collection of archives, we have primarily relied on a full text extraction of webpages provided by the ARCH service and trained our focus on a select number of URLs. The number of Google Colab notebooks we now have has grown quite a bit, based on questions posed by Dr. Duncan Koerber and we encourage you to see the entire collection on our Github page. Some of the more interesting work has involved sentiment analysis and text similarity determination of web page content. Other quantitative questions have also been examined in the notebooks to assist Dr. Koerber in his close readings of the texts, such as when vaccines were first mentioned and how often they were discussed, and how often page content was updated.\nMost of our notebooks rely on popular NLP libraries such as spaCy, scikit-learn, nltk, Gensim, and Flair and make use of Pandas and matplotlib for manipulating data. These libraries, especially when used in conjunction with the Colab platform, allowed us to analyze, display, and share data faster and easier than we would have previously been able to. Technology is progress!\nOne question that Dr. Koerber posed early on was whether Covid-19 content was similar among the twelve municipalities analyzed. To answer this, we turned to using a technique called TD-IDF. Here is a picture of this in action from our notebook. The user selects two URLs to compare, and a similarity score is generated. Words are coloured based on their contribution towards similarity, with brighter colored words having greater significance.\nDr. Koerber also wished to determine the sentiment of tweets by Pelham’s mayor Marvin Junkin, as well as the sentiment of the different municipal webpages. The data was analyzed using three different methods: two of which were based on human derived lexicon and rules (TextBlob, VADER) and one of which was machine learning based (DistilBert). Unfortunately, each method produced quite different results and we interpreted this in part because of the specific training set and lexicon chosen for the methods. Dr. Koerber then compared his close readings to the results to determine the method he considered most appropriate for our data.\nAnalyzing the web archives would certainly have been much more challenging if we did not have the ARCH full-text derivatives, the use of Python and its spectacular libraries, and the Google Colab environment. Dealing with duplicated code among our many notebooks, as well as limitations with the Google Colab environment - in comparison to Jupyter-Lab – posed some issues but were positively outweighed by the speed and convenience of using such an environment. Overall, we feel quite lucky to have been chosen as part of the Archives Unleashed 2021 cohort and given the chance to pour over this interesting dataset.\nAnd, here is a sampling of some of the results we have derived and the notebooks in which they are from. Click the image to see it enlarged and click the link to see the notebook in action.\nMonthly average page size over time\rA heatmap of page update counts\rCount of crawls mentioning vaccines\rCrawls per month by municipality\rMean sentiment scores by domain\rTimeline of first mention of the vaccine by municipality\r","permalink":"https://brockdsl.github.io/archives_unleashed/posts/google-colab-notebooks/","summary":"An overview of our Google Colab notebooks","title":"Our Google Colab notebooks"},{"content":"… we’re nearing the end of year 1 of the Archives Unleashed cohort program. During June 1 to June 3, the AU team, Internet Archive staff, and representatives from cohorts 1 and 2 met in Vancouver, BC for the AU Cohort Program closing event. Our host, the Internet Archive, welcomed us to their new Canada HQ, the Permanent (see below for shots of the exterior and interior).\nThe event began with introductions, and short presentations by cohort 2 teams about their projects. All projects seem super interesting and are varied: from the history of Mormon mommy-bloggers to queer communities in Geocities to COVID19 in Saskatchewan (uh, we’re very interested in the latter).\nSubsequently, representatives from cohort 1 facilitated 1-hour long discussions about their projects. The idea here was not to simply report on progress, but to ask questions and possibly \u0026lsquo;workshop\u0026rsquo; the project with the everyone involved. You can check our slides below:\nDay two featured a panel of cohort 1 representatives discussing a variety of topics, from what we wish we’d known before starting to thoughts about the ethics of using web archives. Following this panel, we disbanded into breakout groups to plan ahead (what does publishing this research look like, and where will we do it?)\nI’m grateful to have had the opportunity to represent our cohort at this event (mountains are really nice!). This was the first in-person event that I’ve attended in several years, and so, I definitely felt some of the rust falling off of the conversational muscles that have atrophied a little during the pandemic. Moreover, this event – after a year of working on a project with minimal interaction with other cohorts – gave me a sense of how our efforts sit in the larger scheme of the AU cohort program. I think applauding successes and commiserating over shared frustrations are key elements of a shared discipline/methodology/scholarly tradition. Working with web archives in this way is very much a nascent way of doing things; this event nudged us in the right direction.\nAnd one last bit of good news: cohort 1 projects have been extended into 2022-23. This means that we’ll continue to be connected with the AU team and cohort 2, and have the time we need to meet our research goals. Stay tuned for more updates.\n","permalink":"https://brockdsl.github.io/archives_unleashed/posts/and_just_like_that/","summary":"A look at the Archives Unleashed closing event held June 1 to June 3 in Vancouver British Columbia.","title":"And just like that…"},{"content":"With the start of 2022 we\u0026rsquo;ve finally come to the point where we need to finish collecting our dataset and now start cleaning it up and analyzing it. One of our primary research goals is to see how different municipalities in the Niagara Region communicated COVID information on their webpages during the unfolding crisis. This of course is made possible by the wonderful dataset we are creating.\nEnter Google Colab Google Collabratory is Google\u0026rsquo;s answer to the Jupyter/IPython notebook. In short it is platform that allows you to create a webpage with embedded code snippets. Since it is build on Google infrastructure it ties in very nicely with Google Accounts, and it offers a Pro option that allows you to tap into more computing power. The great thing about Colab is that you don\u0026rsquo;t need to install any software on your local machine and sharing your results is as simple as collaborating on a Google Doc. One final thing worth pointing you is that you can save itertions of your notebooks directly to GitHub so they are even more accessible to the web. The Digital Scholarship Lab is a huge fan of sharing everything (data, teaching material, workshops) via GitHub.\nPreliminary Results As mentioned we decided that we would collect our web archive until end of December 2021 so that when we returned for the new year we\u0026rsquo;d turn our attention to scafolding up our analysis tools. To start with we restricted our dataset to the 14 municipalities in the Niagara regions. Here\u0026rsquo;s a graph showing how many URLS were captured.\nOnce we narrow down to a city we can drill down even farther to see the details about urls that were harvested. We even calculated some additional metrics on our full-text content:\nPage Length, simply is the number of characters that are in the full-text. When you notice a change to this number that means that content has been added or removed from the page. Sentiment Score, this is a fascinating measure that attempts to quantify the sentiment, or mood expressed, on the page. Specifically we are using the VADER sentiment analysis tool. Plotting these two values for URL captures in the dataset is very satisfying.\nTry it yourself To no surprise, we have shared the notebook on our Github repository for this project. If you add it directly to your own Colab account you render any of the 14 municipalities data. The notebook offers a download of a constructed CSV file of your dataset. Just follow the badge link to begin: ","permalink":"https://brockdsl.github.io/archives_unleashed/posts/municipal_data_notebook/","summary":"A brief look at some of our data, partially prepped and ready to download","title":"Municipal Data with Google Collab"},{"content":"As participants in the Archives Unleashed Cohort Program, our research aims to explore the way Covid-19 has been communicated by municipal governments, businesses, and other organizations in the Niagra region. Exploring the dimensions of crisis communication is a complex undertaking, even more so when numerous actors with varying messages, intentions, motivations, and degrees of style are present. To gain a better understanding of how this communication has occurred, the team will be pouring over 300 GBs of webpages archived during the pandemic.\nOne tool that we are looking to help explore such a large body of documents is Warclight, a web application and frontend for exploring web archives (WARCs) that have been indexed into Apache Solr. Solr is a powerful engine that allows for the efficient location of documents based on full-text search and facets but is difficult to use without firsthand knowledge of its query language. WARCs are an ISO 28500 standard file format developed to facilitate the archival of web pages. Information stored includes metadata such as the date and time of the request and the HTTP response, while the actual data can be HTML, images, videos, and other types of media. In the context of our own collection, we have tens of thousands of WARCs, which correspond to the \u0026ldquo;web crawls” of web pages that we are interested in examining. Warclight is necessary because even though technically these WARCs can be indexed into Solr, searching over the collection can be quite cumbersome without a convenient interface.\nOur current workflow is as follows: Archive-It! is used to archive web crawls of select websites from the Niagra region. This data is available in WARCs and can be downloaded using an exposed API and py-waspi-client. WARCs are then indexed into Solr using the UK Web Archive’s Web Archive Discovery Tool. Our instance of Warclight is hosted on Amazon AWS and is deployed using Ansible, a tool that makes routine system administrative tasks easier and more reliable. Using virtual machines and Ansible together, ensures that deployments are repeatable and fast. And while the setup of Warclight has been relatively straightforward, estimating the amount of space that will be necessary to index our entire collection of WARCs is still unknown. Nevertheless, it should be less than the total size of our WARC collection. Another open question is how long it will take to index the entirety of our WARCs into Solr from scratch.\nComponents and services used Once Warclight has access to indexed WARCs, information discovery occurs in several ways. Facets are very useful for isolating data. Full text search is also useful for locating webpages based on keywords, phrases, or other textual linkages. Once results are found, metadata about an archive can pinpoint a user to further clues, such as when the data was archived, the host that was crawled, outbound links, and a link to a recent archive of the page - accomplished using the Momento Timetravel API.\nAvailable facets Searching in Warclight One perceived drawback of Warclight is that it does not have any methods for displaying the captured data, itself. If an image or video is provided as a search result, the user cannot view this media as it was captured and must replay on a Timetravel generated URL. Similarly for HTML data, only the raw extracted text is available, with the layout, images, CSS, and functionality stripped from the results. Here again, a Timetravel URL is used to view the original page. However, it is important to understand the limitations of using the Timetravel service, as it serves up captures created by other organizations, at temporal times closest to – and most importantly, not necessarily exactly – the time that a WARC was generated. This means that there may be at times inconsistencies with the raw data that Warclight shows and the provided web archive URL by the Timetravel service. Given that we have access to the actual WARCs and Warclight provides a reference to the WARC file that generated a search result, this hopefully should not prove to be a real issue.\nWe hope that Warclight will prove to be a robust and powerful tool for examining our web archives and that it will help use answer questions about how Covid-19 has been communicated within the Niagra region.\nAbout Warclight Warclight is released by the Archives Unleashed Project and was developed by Nick Ruest from York University Libraries, and Ian Milligan and Jimmy Lin from the University of Waterloo. Warclight in turn is based off Blacklight, a frontend interface for Solr, but adds support for browsing indexed WARC data. An online demo of Warclight is available at https://warclight.archivesunleashed.org/, that has several interesting collections to browse.\n","permalink":"https://brockdsl.github.io/archives_unleashed/posts/what-is-warclight/","summary":"An introduction to our use of the Warclight platform","title":"Warclight"},{"content":"At its core, the Crisis Communication in Niagara project is a research endeavor — we are attempting to analyze a segment of the web and derive meaning out of thousands of websites, text documents, images, and other file types. Many of us are also teachers; naturally, we have an interest in the pedagogical implications of presenting the web as a source of data.\nHow do we best introduce students with little or no background to this topic? To scratch this pedagogical itch, members of the team (Tim Ribaric, Cal Murgu, Karen Louise-Smith) developed a course module that reframes the web as a rich data source, rather than simply a medium to browse, explore, and consume. The a/synchronous module was featured in Dr. Karen Louise-Smith’s 4th Data and Society course. The learning objectives of the module were as follows:\nO1: Introduce students to the technical and ethical aspects of web archiving O2: Offer opportunities for students to experience web archives as users and as technicians To meet these objectives, we created the module using a ‘flipped-classroom’ model: students completed an asynchronous lecture and a series of DIY activities in the LMS, followed by a synchronous ‘sensemaking’ session on October 20th.\nAhead of the ‘sensemaking’ session on October 20th, students were asked to read the following articles:\nMcCrow-Young, A. (2021). Approaching Instagram Data: Reflections on Accessing, Archiving, and Anonymising Visual Social Media. Communication Research and Practice, 7(1), 21-34.\nLin, J., Milligan, I., Oard, D. W., Ruest, N., and Shilton, K. (2020). We could, but should we? Ethical considerations for providing access to GeoCities and Other Historical Digital Collections. CHIIR \u0026lsquo;20: Proceedings of the 2020 Conference on Human Information Interaction and Retrieval , 135-144.\nThe recorded lecture (embedded below) covers the basics of web archives, including what they are, how they function, and why we should be interested in preserving the web. The lecture is bookended by the #freedaleaskey story, expertly described by Ian Milligan, Nick Ruest, and Anna St. Onge in their article in Digital Studies.\nFollowing the lecture, students were asked to complete a series of activities to meet the second objective. The first activity invited students to visit the WayBack machine and search for a website that is important to them. To visualize change over time, students had to explore three different snapshots of the same url and describe the changes that they observed in several sentences. As an example, snapshots of the Brock University website from 1999, 2007, and 2013 were shown.\nCirca 1997\rCirca 2007\rCirca 2013\rStudent responses gestured to the interesting information that we can derive from an analysis of how websites can change over time. For instance, a researcher may focus on branding and promotion efforts, or how product launches have changed in relation to the growth of the company.\nThe second activity asked students to assume the role of an analyst by exploring a computational notebook. The computational notebook was created in Google Colab by Tim Ribaric. The notebook features a variety of scripts that analyze crawl rates, top urls by crawl frequency, change of text on a page over time, and sentiment analysis using TextBlob. While students did not have to write any code, they had to run code chunks and interpret results, such as graphs and plots. In the concluding section of the notebook, students were asked to change a variable (the url of a page), and reflect on the subsequent analysis. Without question, this activity represented the most challenging task in this module.\nThe final activity required that students synthesize the entire lesson into a thoughtful response to the following question:\nSuppose that you’ve been asked to create a policy for web archiving Brock University’s Instagram posts given what you’ve read and experienced about Web Archives during this lesson. What are some essential elements that you would consider as you begin to craft this policy?\nIn their responses, students cited interesting ethical issues surrounding Instagram posts, privacy concerns of those featured in posts, infrastructural considerations with respect to crawl frequency and storage, among other topics.\nReflection We learned a lot from this experience. For starters, guest lecturing on the week after reading week presents challenges with sustaining student involvement, particularly if the content is asynchronous and not associated with a gradebook item (such as a participation mark). More importantly, we developed this content fully aware that the Google Colab activity would engender some anxiety among students, specifically those not familiar with running or writing code. While we did not ask students to write new code, the appearance of unfamiliar syntax alone was enough to dissuade participation. Based on participation in the activity and classroom discussion, it looks like the Colab portion was a step too far for some students; however, several students successfully completed the notebook and reflected on the experience.\nIt is one thing to discuss change over time in the abstract and show three examples of change over time (as in the first activity), but quite another to analyze thousands of web pages and demonstrate change over time in a graphical form. What is clear is that we have not figured out how to bridge this technical gap quite yet. The issue may revolve around how content is presented. Nevertheless, a Colab environment is a promising proposition.\n","permalink":"https://brockdsl.github.io/archives_unleashed/posts/teaching-with-web-archives/","summary":"A summary of the in class activity held in conjunction with the project","title":"Teaching with Web Archives"},{"content":"Theory and Questions of the Archives Unleashed Niagara Region COVID-19 Dataset In this Archives Unleashed project, our team will be analyzing the Niagara Region COVID-19 dataset from the perspective of crisis communication theory. Crisis communication is a relatively young field by most standards, extending back just about 30 years. Back then, crisis communication researchers began with small-scale case studies of response messaging from individual organizations. The main question was this: Did the organization respond effectively to a crisis? If not, what could the organization have done better?\nToday, one trend in the field is towards computational analysis of very large datasets to go beyond the study of the individual organizational case. Looking at broader datasets can make eventual findings more significant – it’s not just one case. Our Niagara Region COVID-19 dataset is so big – gigabytes of website and social media data from many regional organizations – that computational analysis allows us to deal with and examine crisis messaging more systematically and on a broader scale. This analysis allows us to ask all kinds of important questions of the dataset and come to new, significant conclusions that further crisis communication theory. In turn, we hope practitioners can use these findings to deal effectively with the next mega crisis.\nBut first we have to ask questions that link past crisis communication theory with our dataset. Past research in the field on these mega crises falls into a few main themes:\nhow crisis communication messaging contributes to community resilience; the development of public trust; education about and compliance with public health recommendations; public engagement with organizations. Researchers have also considered the factor of emotions in public messaging and messaging consistency and congruence during mega crises. With these themes in mind, here are some of the questions we’re thinking about asking of the Niagara Region COVID-19 dataset:\nDid private and public Niagara Region organizations communicate similar messages and advice to the public during the pandemic? Was there “one voice” across the region or diverging voices?\nWas the purpose of communication instructional or emotional?\nIf the messages were instructional, were the instructions consistent across organizations (re. masking, social distancing, vaccination etc.)?\nDid organizational messaging change over time in terms of content and emotional sentiment?\nWere messages to the public simple or complicated?\nHow much of the message content was about organizational status versus community building?\nDid organizations foreground messages from their leaders? Or did they speak as a ‘faceless’ organization?\nDid organizations tell stories about the pandemic or just stick to the facts about COVID-19?\nDid the emotions of the messages change over time, reflecting developments in the pandemic?\nThese questions are exploratory in nature and quite broad at this time. But in the near future, as we start to dig into the dataset, we’ll be able to determine whether the questions can be answered at all. Most likely, computational analysis will favour some questions over others, and we can start moving more fully in a few of these directions. And those findings should provide some interesting, useful guidance for public relations practitioners, perhaps even as the pandemic continues into the winter of 2022.\nDuncan Koerber is the author of Crisis Communication in Canada published by University of Toronto Press, and is available in the Brock Library Collection\n","permalink":"https://brockdsl.github.io/archives_unleashed/posts/crisis-communication-theory/","summary":"An introduction to crisis communication and how it relates to the Niagara COVID-19 archive","title":"Crisis Communication Theory and Questions"},{"content":"\rThe COVID-19 in Niagara Dataset Traditionally in the archival profession, records arrive in archives when they are no longer an active part of the day-to-day functions or needs of the creator. The materials still have informational, evidentiary, or historic value. Thus, these need to be preserved for long term access. Most often, such records come to the archives years after their active life. In today’s digital world, records can change quickly – day to day, even hour by hour. The need to capture and store this information effectively for future reference and study is important.\nWhen the COVID-19 outbreak came to Canada in mid-March 2020, websites became one of the primary sources for information about the virus. The messaging and reactions surrounding COVID-19 seemed to change daily. In April 2020, the Brock University Archives started to capture COVID-19 related webpages of major municipal governments, businesses, and organizations in the Niagara region of Canada using the web archiving tool Archive-It in an effort to save the evolving record of this area’s response to this historic pandemic.\nWe identified 56 key local institutions, governments, and organizations that needed to communicate their pandemic related messages to their clients, constituents, and patrons often. Each of these groups created a COVID-19 webpage on their websites. These webpages have been crawled and saved using Archive-It weekly since April 2020. Our intention is to continue to do so through the duration of all pandemic measures surrounding COVID-19. As of this writing, there has been over 3.3 million documents saved equaling 301 GB of data. Here are some samples:\nTo supplement this information, we have also collected local news stories about the pandemic.\nBecause Archive-It is a paid subscription service, there is a cap on the amount of allotted storage space that Brock gets every year. To manage our allotment, most of these webpages were captured at either the One Page or One Page+ setting depending on the amount of data the creator placed on their websites weekly. For the One Page setting, hyperlinks within the page may not lead to another webpage. For One Page+, the first page as well as the first page of any URLs directly linked off of your seed was archived. We also set a data limit to each webpage to ensure that a few content heavy webpages would not use all the storage space at the detriment of the rest. At the start of the pandemic, the weekly capture of information ranged from 5 - 11 GB. Since October 2020, the average has settled to 2 -3 GB per week.\nAfter managing this dataset over the past year and a half, I cannot wait to see the discoveries that lie within and the fine research that will be done.\n","permalink":"https://brockdsl.github.io/archives_unleashed/posts/the-dataset/","summary":"A brief overview of the dataset we are using","title":"The Dataset"},{"content":"\rThe Project This website is dedicated to documenting the progress of the Brock University team as they work through the Archives Unleashed Cohort program. Over a series of blog posts we are hoping to tell the story of the project and share the outputs of our work.\nArchives Unleashed Archives Unleashed is a huge collaborative initiative that spans many different universities, governmental organizations, the Internet Archive, and the Andrew W. Mellon Foundation. Its primary mission is to make petabytes of historical internet content accessible to researchers interested in studying the recent past.\nArchives Unleashed Cohort program One of the many projects Archives Unleashed is undertaking is a Cohort program. The goal of which is to facilitate research engagement with web archives. The Crisis Communication in Niagara project was a successful application for this grant. As we progress through all of the stages of our work we will document outcomes on this site and hopefully tell a story of not just our end results but all of the steps along the way. In total for this year five projects were selected for the Cohort program. A recent press release provides a great summary of those five.\nKick off Event Work officially began in mid-July with a start up event where all five groups presented their researcher plans. I encourage you to take a look at our slides for more details about what we are hoping to tackle. The two day session also included lots of chances for generative discussions about utilizing web archives in research, and a demo of a new tool project teams will be able to use to create derivatives from their larger web archives.\nWatch this space for more updates as we progress. Next step up is getting our research assistant lined up and preparing for an in-class activity ready for October.\n","permalink":"https://brockdsl.github.io/archives_unleashed/posts/introduction_to_the_project/","summary":"The Project This website is dedicated to documenting the progress of the Brock University team as they work through the Archives Unleashed Cohort program. Over a series of blog posts we are hoping to tell the story of the project and share the outputs of our work.\nArchives Unleashed Archives Unleashed is a huge collaborative initiative that spans many different universities, governmental organizations, the Internet Archive, and the Andrew W.","title":"An introduction to the project"},{"content":"Our goal Using web archives collected by Brock University, this project will examine how organizations in the Niagara region have responded to government COVID-19 mandates. Analysis will focus on investigating three types of entities: local government, non-profit organizations, and major private entities. Findings from this research aim to inform future crisis communication organizational planning, specifically at the local and municipal level. The project will also create several open computational notebooks to support teaching, learning, and research.\nProject Team Tim Ribaric\rDavid Sharron\rCal Murgu\rKaren Louise Smith\rDuncan Koerber\rResearch Assistants Victoria Danh\rFletcher Johnson\rThe Collection Datasets used to complete this project will mainly be derived from the COVID19 in Niagara Web Archive collection. This collection performs a weekly crawl of websites of major institutions, governments and organizations in the Niagara area focusing on the varied responses to the COVID-19 pandemic in 2020. Further, one time crawls of newspaper articles focused on Niagara centric COVID-19 news. It is managed by the Brock University Archives and Special Collections.\nMethods Using Archive-It analysis tools, the team will create derivative files, such as full-text derivates, or url derivates, which will later be explored using other methods. Methods include text analysis techniques, such as sentiment and word frequency/tf-idf analysis, topic modelling to determine topic clusters, and close reading techniques.\nPartners This project is made possible by financial and technical support from the folks at the Archives Unleashed Project:\nIan Milligan (University of Waterloo)\nJefferson Bailey (Internet Archive)\nNick Ruest (York University)\nJimmy Lin (University of Waterloo)\nSamantha Fritz (University of Waterloo)\n","permalink":"https://brockdsl.github.io/archives_unleashed/about/","summary":"Our goal Using web archives collected by Brock University, this project will examine how organizations in the Niagara region have responded to government COVID-19 mandates. Analysis will focus on investigating three types of entities: local government, non-profit organizations, and major private entities. Findings from this research aim to inform future crisis communication organizational planning, specifically at the local and municipal level. The project will also create several open computational notebooks to support teaching, learning, and research.","title":"About"},{"content":"Code and Repositories ARCH Data Explore - A github repository that has some preliminary data exploration notebooks.\nIn the News October 5, 2021\nThe web as data: How and why we archive the web Youtube\nNovember 9, 2021\nBrock Research team analyhing how Niagara communicated COVID crisis Brock News\nNovember 13, 2021\nBrock examing Niagara\u0026rsquo;s crisis communications during COVID-19 St. Catharines Standard\nFebruary 9, 2022\nLibrary as Laboratory: A New Series Exploring the Computational Us of Internet Archive Collections. Internet Archive\nFebruary 10, 2022\nResearch Applications with Web Archives: Collaboration Among Archives Unleashed Cohorts. Archives Unleashed\n","permalink":"https://brockdsl.github.io/archives_unleashed/results/","summary":"Code and Repositories ARCH Data Explore - A github repository that has some preliminary data exploration notebooks.\nIn the News October 5, 2021\nThe web as data: How and why we archive the web Youtube\nNovember 9, 2021\nBrock Research team analyhing how Niagara communicated COVID crisis Brock News\nNovember 13, 2021\nBrock examing Niagara\u0026rsquo;s crisis communications during COVID-19 St. Catharines Standard\nFebruary 9, 2022\nLibrary as Laboratory: A New Series Exploring the Computational Us of Internet Archive Collections.","title":"Results"}]